{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following program is meant to maximize $\\mu.w - \\kappa w.\\omega w$ over w (weight vector) for a given return vector $\\mu$, risk aversion $\\kappa$ and covariance matrix $\\omega$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it can be conveniently done using the technique of Lagrange multiplier (we get analytical solution), here we do this using gradient descent. This method might have advantage once we add constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm running...\n",
      "i = 0,  \tw = [ 0.50000801  0.49999199],  \tVal = 0.059800214741997114\n",
      "i = 10000,  \tw = [ 0.56203418  0.43796582],  \tVal = 0.06144404459973235\n",
      "i = 20000,  \tw = [ 0.59778824  0.40221176],  \tVal = 0.0623748245823386\n",
      "i = 30000,  \tw = [ 0.61808091  0.38191909],  \tVal = 0.06289764145294786\n",
      "i = 40000,  \tw = [ 0.62949976  0.37050024],  \tVal = 0.06319009670251428\n",
      "i = 50000,  \tw = [ 0.6358947  0.3641053],  \tVal = 0.0633533345421052\n",
      "i = 60000,  \tw = [ 0.63946661  0.36053339],  \tVal = 0.06344434059413875\n",
      "i = 70000,  \tw = [ 0.64145877  0.35854123],  \tVal = 0.06349504430671037\n",
      "i = 80000,  \tw = [ 0.64256896  0.35743104],  \tVal = 0.06352328367562265\n",
      "i = 90000,  \tw = [ 0.64318735  0.35681265],  \tVal = 0.06353900846490144\n",
      "i = 100000,  \tw = [ 0.64353172  0.35646828],  \tVal = 0.06354776368977363\n",
      "i = 110000,  \tw = [ 0.64372347  0.35627653],  \tVal = 0.0635526381154095\n",
      "i = 120000,  \tw = [ 0.64383023  0.35616977],  \tVal = 0.06355535183488832\n",
      "i = 130000,  \tw = [ 0.64388966  0.35611034],  \tVal = 0.06355686260473614\n",
      "i = 140000,  \tw = [ 0.64392275  0.35607725],  \tVal = 0.06355770366503186\n",
      "i = 150000,  \tw = [ 0.64394117  0.35605883],  \tVal = 0.06355817188877347\n",
      "i = 160000,  \tw = [ 0.64395143  0.35604857],  \tVal = 0.06355843255113526\n",
      "i = 170000,  \tw = [ 0.64395713  0.35604287],  \tVal = 0.06355857766283825\n",
      "i = 180000,  \tw = [ 0.64396031  0.35603969],  \tVal = 0.06355865844698015\n",
      "i = 190000,  \tw = [ 0.64396208  0.35603792],  \tVal = 0.06355870341974128\n"
     ]
    }
   ],
   "source": [
    "### We implement a gradient descent to solve portfolio optimization problem\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Some constants for the algorithm\n",
    "max_number_of_iterations = 1000000\n",
    "learning_rate = 0.01\n",
    "\n",
    "## Function to rescale weights to 1 everytime we run our algorithm\n",
    "def rescale_weights_to_1(w) :\n",
    "    new_w = np.array([x / sum(w) for x in w])\n",
    "    return new_w\n",
    "\n",
    "#w = [1,1,1,0]\n",
    "#a = rescale_weights_to_1(w)\n",
    "#a\n",
    "\n",
    "## Function to calculate gradient wrt weight\n",
    "def gradient_wrt_weight(mu, sigma, k, w):\n",
    "    ones = np.array([])\n",
    "    ones = np.zeros(len(w)) + 1\n",
    "    gradient = mu*ones - 2*k* np.dot(sigma, w.T)\n",
    "    error = -1*(np.dot(mu,w) - k * np.dot(w,np.dot(sigma,w.T)))\n",
    "    gradient = gradient*error\n",
    "    # print(\"Gradient: \", gradient)\n",
    "    return gradient\n",
    "\n",
    "## Function to calculate error\n",
    "## We use square function to ensure positive slope\n",
    "def error_given_weight(mu, sigma, k, w):\n",
    "    error = -1*((np.dot(mu,w) - k * np.dot(w,np.dot(sigma,w.T))) ** 2)\n",
    "    # print(\"Error: \", error)\n",
    "    return error\n",
    "\n",
    "## We also need a switching function to determine if we need to switch to new weight\n",
    "def switch(mu, sigma, k, w1, w2):\n",
    "    y1 = error_given_weight(mu, sigma, k, w1)\n",
    "    y2 = error_given_weight(mu, sigma, k, w2)\n",
    "    if (y1 > y2) :\n",
    "        return 1\n",
    "    else :\n",
    "        return 0\n",
    "    \n",
    "## The function to calculate stepwise gradient \n",
    "## If the switching does not happen we decrease learning rate by half\n",
    "def step_gradient(mu, sigma, k, w) :\n",
    "    global learning_rate\n",
    "    y = w - (learning_rate * gradient_wrt_weight(mu, sigma, k, w))\n",
    "    # print(\"y: \",y)\n",
    "    if(switch(mu, sigma, k, w, y) == 1):\n",
    "        new_w = rescale_weights_to_1(y)\n",
    "        # print(new_w)\n",
    "    else:\n",
    "        learning_rate = learning_rate*0.5\n",
    "        new_w = w\n",
    "    return new_w\n",
    "\n",
    "## define initial weight\n",
    "def initial_weight(mu):\n",
    "    #w_init = []\n",
    "    #for i in range(len(mu)) :\n",
    "    #    w_init.append(1)\n",
    "    return (rescale_weights_to_1(np.zeros(len(mu))+1))\n",
    "\n",
    "## Running Gradient Descent Algorithm\n",
    "def gradient_descent(mu, sigma, k, number_of_iterations):\n",
    "    print(\"Algorithm running...\")\n",
    "    w = initial_weight(mu)\n",
    "    if(number_of_iterations > max_number_of_iterations) :\n",
    "        number_of_iterations = max_number_of_iterations\n",
    "    for i in range(0, number_of_iterations):\n",
    "        w = step_gradient(mu, sigma, k, w)\n",
    "        if(i%10000 == 0):\n",
    "            print(\"i = {0},  \\tw = {1},  \\tVal = {2}\".format(i, w, (np.dot(mu,w) - k * np.dot(w,np.dot(sigma,w.T)))))\n",
    "    return w\n",
    "\n",
    "mu = np.array([0.09, 0.06])\n",
    "sigma = np.array([[0.045,0.035], [0.035, 0.037]])\n",
    "k = 0.4\n",
    "\n",
    "w= gradient_descent(mu, sigma, k, 200000)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
